{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f75bd56",
   "metadata": {},
   "source": [
    "<h1><u> Implementation of 3-NN</h></u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e979ae3f",
   "metadata": {},
   "source": [
    "<b><i>Import the following libraries:\n",
    "- numpy\n",
    "- sys\n",
    "- math</b></i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20e1dbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3833d3b6",
   "metadata": {},
   "source": [
    "<b>Import Iris dataset from sklearn.datasets</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a45a7884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987ba224",
   "metadata": {},
   "source": [
    "Import Ionosphere dataset from local environment.\n",
    "Here, path = ' < dummy path > '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c811750",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.genfromtxt(\"< dummy path >\", delimiter=\",\", usecols=np.arange(34))\n",
    "y = np.genfromtxt(\"< dummy path >\", delimiter=\",\", usecols=34, dtype='int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f596c752",
   "metadata": {},
   "source": [
    "<b>Split Iris and Ionosphere datasets using train_test_split.\n",
    "- For Iris dataset, X_train_IR, X_test_IR, y_train_IR, y_test_IR are used.\n",
    "- For Ionosphere dataset, X_train_IO, X_test_IO, y_train_IO, y_test_IO are used.\n",
    "- Birthdate = 03/09, \n",
    "  <br>  therefore random_state = 39 and 309 respectively (by omitting leading zeroes from date and month)</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d16e1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_IR, X_test_IR, y_train_IR, y_test_IR = train_test_split(iris.data, iris.target, random_state=39) # Here, zeroes are removed as the response for 3 NN algorithm was not proper.\n",
    "X_train_IO, X_test_IO, y_train_IO, y_test_IO = train_test_split(X, y, random_state=309)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d783973d",
   "metadata": {},
   "source": [
    "<b>The below code contains the implementation of 3-NN algorithm on IRIS dataset. \n",
    "- First step is to calculate euclidean distance between training and test sample points.\n",
    "- The results are stored in distances.\n",
    "- Next step is to find the three least distances for each test sample with the respective training set, and returns the list of 3 least values.\n",
    "- Further, the indices of the least locations are fetched and accordingly the labels are set<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6121fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance_for_3nn(vec1,vec2):\n",
    "    dis = np.empty(len(vec1))\n",
    "    for inc in range(len(vec1)):\n",
    "        dis[inc] = ((vec1[inc] - vec2[inc]) ** 2)\n",
    "    result = np.sum(dis) ** 0.5\n",
    "    return(result)\n",
    "distances = np.empty(len(X_test_IR)*len(X_train_IR)).reshape(len(X_test_IR),len(X_train_IR))\n",
    "    \n",
    "for i in range(len(X_test_IR)):\n",
    "    for j in range(len(X_train_IR)):\n",
    "        distances[i,j] = euclidean_distance_for_3nn(X_test_IR[i],X_train_IR[j]) \n",
    "        \n",
    "def least_three_numbers(all_distances):\n",
    "    min = sys.maxsize\n",
    "    index=[x for x in all_distances]\n",
    "    mimimum_list = [1,2,3]\n",
    "    for i in range(len(index)):\n",
    "        if index[i] < min:\n",
    "            min = index[i]\n",
    "            m1 = i\n",
    "    mimimum_list[0]=m1\n",
    "    del(index[m1])\n",
    "    \n",
    "    min = sys.maxsize\n",
    "    for i in range(len(index)):\n",
    "        if index[i] < min:\n",
    "            min = index[i]\n",
    "            m1 = i\n",
    "    mimimum_list[1]=m1\n",
    "    del(index[m1])\n",
    "    \n",
    "    min = sys.maxsize\n",
    "    for i in range(len(index)):\n",
    "        if index[i] < min:\n",
    "            min = index[i]\n",
    "            m1 = i\n",
    "    mimimum_list[2]=m1\n",
    "    return mimimum_list        \n",
    "\n",
    "predictions_three = np.empty(len(X_test_IR))\n",
    "for i in range(len(X_test_IR)):\n",
    "    prediction_index=least_three_numbers(distances[i])\n",
    "    dictionary={}\n",
    "    for j in y_train_IR:\n",
    "        dictionary[j]=0\n",
    "    for k in prediction_index:\n",
    "        dictionary[y_train_IR[k]]+=1\n",
    "    list1=[0]\n",
    "    for a1,b1 in dictionary.items():\n",
    "        list1.append(b1)\n",
    "    \n",
    "    if(list1[-1]==1):\n",
    "        predictions_three[i] = y_train_IR[prediction_index[0]]\n",
    "        break\n",
    "    for a2,b2 in dictionary.items():\n",
    "        if(b2==list1[-1]):\n",
    "            predictions_three[i] = a2   \n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce589ee",
   "metadata": {},
   "source": [
    "<b>Print the predictions along with accuracy rate, error count and error rate for IRIS dataset for 3 nearest neighbours.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cca3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Insights for Iris test dataset for 3 nearest neighbours:\\n')\n",
    "print(f'Labels for all the elements of the test set: \\n{predictions_three_IR}')\n",
    "print(f'\\n Error count: {sum(predictions_three!=y_test_IR)} ')\n",
    "print(f'\\n Accuracy rate: {1-(error_count/len(X_test_IR))}')\n",
    "print(f'\\n Error rate: {(error_count/len(X_test_IR))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c55f218",
   "metadata": {},
   "source": [
    "<b>The below code contains the implementation of 3-NN algorithm on IONOSPHERE dataset. \n",
    "- First step is to calculate euclidean distance between training and test sample points.\n",
    "- The results are stored in distances.\n",
    "- Next step is to find the three least distances for each test sample with the respective training set, and returns the list of 3 least values.\n",
    "- Further, the indices of the least locations are fetched and accordingly the labels are set<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f929d90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance_for_3nn(vec1,vec2):\n",
    "    dis = np.empty(len(vec1))\n",
    "    for inc in range(len(vec1)):\n",
    "        dis[inc] = ((vec1[inc] - vec2[inc]) ** 2)\n",
    "    result = np.sum(dis) ** 0.5\n",
    "    return(result)\n",
    "distances = np.empty(len(X_test_IO)*len(X_train_IO)).reshape(len(X_test_IO),len(X_train_IO))\n",
    "    \n",
    "for i in range(len(X_test_IO)):\n",
    "    for j in range(len(X_train_IO)):\n",
    "        distances[i,j] = euclidean_distance_for_3nn(X_test_IO[i],X_train_IO[j]) \n",
    "\n",
    "# find least of three        \n",
    "        \n",
    "def least_three_numbers(all_distances):\n",
    "    min = sys.maxsize\n",
    "    index=[x for x in all_distances]\n",
    "    mimimum_list = [1,2,3]\n",
    "    for i in range(len(index)):\n",
    "        if index[i] < min:\n",
    "            min = index[i]\n",
    "            m1 = i\n",
    "    mimimum_list[0]=m1\n",
    "    del(index[m1])\n",
    "    min = sys.maxsize\n",
    "\n",
    "    for i in range(len(index)):\n",
    "        if index[i] < min:\n",
    "            min = index[i]\n",
    "            m1 = i\n",
    "    mimimum_list[1]=m1\n",
    "    del(index[m1])\n",
    "    min = sys.maxsize\n",
    "\n",
    "    for i in range(len(index)):\n",
    "        if index[i] < min:\n",
    "            min = index[i]\n",
    "            m1 = i\n",
    "    mimimum_list[2]=m1\n",
    "    return mimimum_list        \n",
    "\n",
    "predictions_three_IO = np.empty(len(X_test_IO))\n",
    "for i in range(len(X_test_IO)):\n",
    "    prediction_index=least_three_numbers(distances[i])\n",
    "    dictionary={}\n",
    "    for j in y_train_IO:\n",
    "        dictionary[j]=0\n",
    "    for k in prediction_index:\n",
    "        dictionary[y_train_IO[k]]+=1\n",
    "    list1=[0]\n",
    "    for a1,b1 in dictionary.items():\n",
    "        list1.append(b1)\n",
    "    \n",
    "    if(list1[-1]==1):\n",
    "        predictions_three_IO[i] = y_train_IO[prediction_index[0]]\n",
    "        break\n",
    "    for a2,b2 in dictionary.items():\n",
    "        if(b2==list1[-1]):\n",
    "            predictions_three_IO[i] = a2   \n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059b19d8",
   "metadata": {},
   "source": [
    "<b>Print the predictions along with accuracy rate, error count and error rate for IONOSPHERE dataset for 3 nearest neighbours.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6c82cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Insights for Iris test dataset for 3 nearest neighbours:\\n')\n",
    "print(f'Labels for all the elements of the test set: \\n{predictions_three_IO}')\n",
    "print(f'\\n Error count: {sum(predictions_three_IO!=y_test_IO)} ')\n",
    "print(f'\\n Accuracy rate: {1-(error_count/len(X_test_IO))}')\n",
    "print(f'\\n Error rate: {(error_count/len(X_test_IO))}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
